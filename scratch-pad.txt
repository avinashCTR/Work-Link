/data1/archive/avinash/guardrailsConfChanges/phrasesMergedIntoRightWordsFashion.csv
/data1/searchengine/rawdata/historydata/jiomart/dump/query_level_data
/data1/archive/avinash/guardrailsConfChanges/beauty_added_v7_v8.csv
/data1/archive/avinash/guardrailsConfChanges/home&lifestyle_min_product_count_20_30.csv
/data1/archive/avinash/guardrailsConfChanges/other_vertical_min_product_count_known_words_100_200.csv

package ai.couture.obelisk.commons.maths.evaluationmetrics.rankingsystem


/data1/searchengine/processed/jiomart/accumulateddata/IPATransliterationsAutomaticAccumulator
.agg(SpearmCorrelation.SpearmanCorr("c1", "c2").toColumn.as("correlation"))

notebooks/avinash/rankingAnalysis/ranking-analysis.ipynb

http://10.166.181.219:8889/lab/tree/notebooks/avinash/GaurdrailsAnalysisTasks/BrandNonBrand.ipynb

hdfs://10.144.96.170:8020/data1/archive/avinash/guardrailsTemp/BrandNonBrand

split(regexp_replace(col("entity"), "[\\[|\"|\\]]", ""), ",")\

dev07052025_1205

/data1/archive/avinash/Template-Testing/ajio/output


package ai.couture.obelisk.search.etl.fetch

import ai.couture.obelisk.commons.Constants.NUM_PARTITIONS
import ai.couture.obelisk.commons.Constants.REGEX_PATTERN.{HTML_TAG, IS_NUM_ONLY}
import ai.couture.obelisk.commons.io._
import ai.couture.obelisk.commons.utils.{BaseBlocks, DataFrameUtil}
import ai.couture.obelisk.commons.utils.arrayutil.Array1D.strReplaceInArray
import ai.couture.obelisk.search.Constants._
import org.apache.spark.sql.expressions.UserDefinedFunction
import org.apache.spark.sql.functions._
import org.apache.spark.sql.types.{ArrayType, StringType}
import org.apache.spark.sql.{DataFrame, SparkSession}

import scala.collection.mutable.WrappedArray
import javax.sound.midi.Sequence

object DeriveL1LevelCatalogueData extends BaseBlocks {

  var catalogueAttributesData: DataFrame = _

  
  val getL1Names: UserDefinedFunction = udf { arr: Seq[String] =>
    arr.map(_.split(":")(0)).toSet.toSeq
  }

  def load(): Unit = {
    catalogueAttributesData = ParquetToDF.getDF(setInputPath("unified_catalogue"))
  }

  def doTransformations(): Unit = {
    catalogueAttributesData = catalogueAttributesData.select(L1_LEVEL_CATALOGUE_COLUMNS.map(col): _*)
    catalogueAttributesData = catalogueAttributesData.withColumn("l1_name", getL1Names(col("l1l2l3_category")))
    catalogueAttributesData = catalogueAttributesData.withColumn("l1_name", explode(col("l1_name")))
  }

  def save(): Unit = {
    catalogueAttributesData.repartition(col("vertical_code")).write.partitionBy("vertical_code").option("maxRecordsPerFile", 100000).parquet(setInputPath("split_l1_level_catalogue"))
  }

}