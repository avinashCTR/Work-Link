from airflow import DAG
from airflow.contrib.operators.ssh_operator import SSHOperator
from datetime import datetime, timedelta

from CoutureSpark3Plugin import CoutureSpark3Operator

import textwrap

code_artifact="couture-search-pipelines-3.0.0-avinash.jar"
classPath = 'ai.couture.obelisk.search.MainClass'

default_args = {
    'owner': 'couture',
    'depends_on_past': False,
    'start_date': datetime(2020, 1, 1),
    'retries':0,
    'config_group':"config_group_jiomart"
}

############################################### Paths and Configs ################################################################
modelPath = "/data1/searchengine/EmbeddingTrainingDataCuration/ajio/TrainedModels/mpnet_v3_ep3"
testDataPath = "/data1/archive/avinash/ModelTestingData2"
outputPrefix = "MpnetOutputs"


################################################ Main Dag ########################################################################

dag = DAG('search_engine_embeddings_compare', default_args=default_args,schedule_interval=None)

env_command = "conda activate /data/archita/searchengine_nlp"

################################################ SpellCheck Testing ##############################################################
"""
In this tasks the inputs are 
1. Model
2. SpellCheck and WordBreak data merged into one , here we have three columns
   query,mistake_queries,category

the output will be a csv report with the metrics on simmilarity score the model produces on the two query columns
"""
bash_command = textwrap.dedent(
    f"""
    {env_command} && sh /app/notebooks/avinash/ModelTestingDag/EmbeddingsCompare.sh \
    --model_path "{modelPath}" \
    --query_file_path "{testDataPath}/inputs/spellCheckAndWordBreak.csv" \
    --report_csv_path "{testDataPath}/outputs/{outputPrefix}/spellCheckAndWordBreakReport.csv" \
    --query_col1 "query" \
    --query_col2 "mistake_queries" \
    --n 20000 \
    --model_name "mpnet_v3_ep3" \
    --batch_size 1024 \
    --step_size 100000 \
    --multi_gpu True
    """
)
SpellCheck = SSHOperator(
    task_id="SpellCheck",
    ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
    command=f"""ssh -p 8509 jioapp@10.166.181.219 "{bash_command}" """,
    dag=dag
)

############################################### SearchStability #################################################################
"""
In this tasks the inputs are 
1. Model
2. UKUS, Hinglish and Shuffled queries data merged into one , here we have three columns
   query,mistake_queries,category

the output will be a csv report with the metrics on simmilarity score the model produces on the two query columns
"""
bash_command = textwrap.dedent(
    f"""
    {env_command} && sh /app/notebooks/avinash/ModelTestingDag/EmbeddingsCompare.sh \
    --model_path "{modelPath}" \
    --query_file_path "{testDataPath}/inputs/SearchStability.parquet" \
    --report_csv_path "{testDataPath}/outputs/{outputPrefix}/SearchStabilityReport.csv" \
    --query_col1 "query" \
    --query_col2 "mistake_queries" \
    --n 200000 \
    --model_name "mpnet_v3_ep3" \
    --batch_size 1024 \
    --step_size 100000 \
    --multi_gpu True
    """
)
SearchStabality = SSHOperator(
    task_id="SearchStability",
    ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
    command=f"""ssh -p 8509 jioapp@10.166.181.219 "{bash_command}" """,
    dag=dag
)

SearchStabality.set_upstream(SpellCheck)

########################################### ZSR analysis #####################################################################
"""
In this tasks from qdrant cache , we get the queries which give zero resulsts that is zsr
the inputs are :-
1.Dataframe with query and backend response column in json( dumped into string )
2.a key in the string json , which should be array and this checks for empty array
the output will be :-
1.Dataframe with same schema but only zsr queries
"""
AnalyseZSRold = None
for experiment in ["default", "non_premium", "premium"]:
    AnalyseZSRnew = CoutureSpark3Operator(
        task_id=f"AnalyseZSR.{experiment}",
        class_path=classPath,
        method_id="AnalyseZSR",
        code_artifact=code_artifact,
        method_args_dict={
            "search_response_column": "response",
            "key_to_check": "top_colorGroup_strings",
        },
        input_base_dir_path="/data1/archive/avinash/QdrantCache",
        output_base_dir_path=testDataPath,
        input_filenames_dict={
            "qdrant_cache": f"{experiment}_top5k_responses"
        },
        output_filenames_dict={
            "zsr_queries": f"/ZSRQueries/{experiment}"
        },
        dag=dag,
        description="filters zsr queries from the qdrant cache"
    )
    if AnalyseZSRold is not None:
    	AnalyseZSRnew.set_upstream(AnalyseZSRold)
    AnalyseZSRold = AnalyseZSRnew

########################################### CTR analysis #####################################################################
"""
In this task, we get the queries from qdrant cache which have click-through rate (CTR) data.
The inputs are:-
1. Dataframe with query and backend response column in json (dumped into string)
2. A key in the string json, which should contain the CTR data
The output will be:-
1. Dataframe with the same schema but only queries that have valid CTR data.
"""

AnalyseCTRold = None
for experiment in ["default", "non_premium", "premium"]:
    AnalyseCTRnew = CoutureSpark3Operator(
        task_id=f"AnalyseCTR.{experiment}",
        class_path=classPath,
        method_id="AnalyseCTR",
        code_artifact=code_artifact,
        method_args_dict={
            "search_term_column": "query",
            "product_id_column": "option_code",
        },
        input_base_dir_path="/data1/archive/avinash/QdrantCache",
        output_base_dir_path=testDataPath,
        input_filenames_dict={
            "history_data": f"interactions",
            "console_data": f"{experiment}_top5000_onlyCodeStrings",
            "top_percentile":0.95,
            "top_n":5000
        },
        output_filenames_dict={
            "ctr_queries": f"/CTR_reports/{experiment}" 
        },
        dag=dag,
        description="Gives the average ctr at query level"
    )
    if AnalyseCTRold is not None:
        AnalyseCTRnew.set_upstream(AnalyseCTRold)
    AnalyseCTRold = AnalyseCTRnew
    
######################################################### Gaurdrails Brand Testing and Template testing ################################
# gpu commands
brand_testing_env_command = "conda activate /data/archita/searchengine_nlp"
brand_testing_bash_command = textwrap.dedent(
        f"""
        {brand_testing_env_command} && sh /app/notebooks/avinash/Gaurdrails_task/Brand-Testing/build.sh \
        --brands_path "/data1/searchengine/processed/ajio/09022025/etl/Brands" \
  		--endpoint "latest_stable" \
  		--disable_spell_check "True" \
        --ouput_path "/data1/archive/avinash"
        """
)


brand_testing = SSHOperator(
    task_id="Brands_Testing",
    ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
    command=f"""ssh -p 8509 jioapp@10.166.181.219 "{brand_testing_bash_command}" """,
    dag=dag
)

template_testing_env_command = "conda activate /home/jioapp/anaconda3/envs/searchengine_phase2"
template_testing_bash_command = textwrap.dedent(
        f"""
        {template_testing_env_command} && sh /app/notebooks/avinash/Gaurdrails_task/Template-Testing/ajio/Template_Testing.sh \
        --templates_path "/data1/archive/avinash/GuardRails_TemplateQueries" \
		--output_base_path "/data1/archive/avinash/Template-Testing/ajio" \
        --vertical "ajio" \
		--phase "dev09022025_1802" \
		--top_n 15
        """
)

template_testing = SSHOperator(
    task_id="Template_Testing",
    ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
    command=f"""ssh -p 8509 jioapp@10.166.181.219 "{template_testing_bash_command}" """,
    dag=dag
)

####################avinash#################################Gaurdrails and jiomart###############################################################

verticals=['groceries','beauty','fashion','home_lifestyle','other_verticals','electronics']

vertical_tasks=[]
for vertical in verticals:
    jiomart_template_testing_env_command = "conda activate /home/jioapp/anaconda3/envs/searchengine_phase2"
    jiomart_template_testing_bash_command = textwrap.dedent(
            f"""
            {jiomart_template_testing_env_command} && sh /app/notebooks/avinash/Gaurdrails_task/Template-Testing/jiomart/Template_Testing.sh \
            --templates_path "/data1/searchengine/testing/jiomart/Templates_testing/{vertical}/Input" \
            --output_base_path "/data1/searchengine/testing/jiomart/Templates_testing/{vertical}" \
            --vertical "{vertical}" \
            --phase "dev0509_1003:{vertical}" \
            --top_n 15
            """
    )

    jiomart_template_testing = SSHOperator(
        task_id=f"Jiomart_Template_Testing.{vertical}",
        ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
        command=f"""ssh -p 8509 jioapp@10.166.181.219 "{jiomart_template_testing_bash_command}" """,
        dag=dag
    )
    
    if len(vertical_tasks)==0:
      template_testing >> jiomart_template_testing
    else:
      vertical_tasks[-1] >> jiomart_template_testing
    
    vertical_tasks.append(jiomart_template_testing)

brand_testing >> template_testing
