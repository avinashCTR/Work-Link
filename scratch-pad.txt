from airflow import DAG
from airflow.contrib.operators.ssh_operator import SSHOperator
from datetime import datetime, timedelta

from CoutureSpark3Plugin import CoutureSpark3Operator
from airflow.models import Variable
import textwrap
from CouturePythonDockerPlugin import CouturePythonDockerOperator
from airflow.utils.task_group import TaskGroup

code_artifact = "couture-search-pipelines-3.0.0-avinash.jar"
classPath = "ai.couture.obelisk.search.MainClass"

default_args = {
    "owner": "couture",
    "depends_on_past": False,
    "start_date": datetime(2020, 1, 1),
    "retries": 0,
    "config_group": "config_group_jiomart",
}

############################################### Paths and Configs ################################################################
modelPath = (
    "/data1/searchengine/EmbeddingTrainingDataCuration/jiomart/TrainedModels/jiomart_ge_oversampled_ep3"
)
testDataPath = "/data1/archive/rohith/ModelTestingData2"
outputPrefix = "MpnetOutputs"


################################################ Main Dag ########################################################################

dag = DAG(
    "search_engine_analysis_new_jiomart",
    default_args=default_args,
    schedule_interval=None,
)

env_command = "conda activate /data/archita/searchengine_nlp"


# old python image -- working for tasks other than spell check
# python_image = "couture/python-search-image:1.0.7"

python_image = "couture/python-search-image:1.1.4"
python_image_new = (
    "couture/python-search-image:1.1.4"  # using this for spell check analysis
)

kerberos_hosts = Variable.get("kerberos_hosts", deserialize_json=True)

code_artifact_python = "__main__automated_search_analysis.py"

commons_egg = "obelisk-search-1.0.1-py3.13.egg"
# python_egg_monil = "couture_search_monil_ajio-2.0.0-py3.12.egg"
python_egg_meghana = "couture_search-2.0.0-py3.10-rohith.egg"  # "couture_search-2.0.0_meghana_topn-py3.10.egg"
python_egg_rohith_gt = "couture_search-2.0.0-py3.13-rohith-gt.egg"

# spell_check_lumos_egg = "lumos-1.1.0-py3.11.egg"
spell_check_lumos_egg = "lumos-1.1.1-py3.12.egg"


required_eggs = [commons_egg, python_egg_rohith_gt, spell_check_lumos_egg]

query_count_all = 10000
searchEngineVolume = "searchEngineVolume:/home/pythondockervolume:Z"

CorpusPath = "/data1/searchengine/processed/ajio/24032025/V1_delta_changes/"
TemplateQueriesDF = "/data1/archive/rohith/28042025/Template_Queries"

DATE = "28042025"
str_date = datetime.now().strftime("%Y%m%d")

dirPathAjio = "/data1/searchengine/processed/ajio/"
dirCummulativePath = f"{dirPathAjio}accumulateddata/"

GuardrailsPath = "/data1/archive/rohith/28042025/Template_Queries_Guardrails/"
Final_Output = GuardrailsPath + "Final_Output/"

with TaskGroup("GuardrailsAnalysis", dag=dag) as GuardrailsAnalysis:
    """
    This DAG is used to analyze the guardrails performance and prepare the guardrails for analysis.
    
    Takes in the template queries, corpus path and other paths required for guardrails
    And returns the response for a single query
    
    contains query -> [res1,....res7] from the guardrails response
    """

    GetGuardrailsResponse = CouturePythonDockerOperator(
        task_id="GetGuardrailsResponse",
        image=python_image,
        api_version="auto",
        auto_remove=False,
        command="/bin/bash echo 'heloword'",
        extra_hosts=kerberos_hosts,
        user="couture",
        dag=dag,
        mem_limit="18g",
        code_artifact=code_artifact_python,
        python_deps=required_eggs,
        method_id="top_nstep_wise_output",
        method_args_dict={
            "n": query_count_all,
            "query_column": "ProductInfo",
        },  # "search_term"},
        volumes=[searchEngineVolume],
        input_base_dir_path=CorpusPath + "AnalysisCorpusJSON/",
        output_base_dir_path=f"/data1/archive/rohith/{DATE}",
        input_filenames_dict={
            "all_queries_path": TemplateQueriesDF,
            "r2syn_path": "r2syn.json",
            "r2sub_path": "r2sub.json",
            "w2r_path": "w2r.json",
            "r2e_path": "r2e.json",
            "r2l1l2_path": "r2l1l2.json",
            "r2l1l3_path": "r2l1l3.json",
            "r2_token_type_path": "r2tokentype.json",
            "r2_isCategory_path": "r2iscategory.json",
            "phrase_corpus_path": "phrases.json",
            "num_corpus_path": "numericalranges.json",
            "entity_corpus_path": "wordentities.json",
            "custom_splitter_vocab_path": f"{dirCummulativePath}splitter_vocab/wordninja_words.txt.gz",
        },
        output_filenames_dict={
            "month_data_out": Final_Output
            + "run_"
            + str_date
            + "/top_"
            + str(query_count_all)
            + "_stepwise_output.csv",
            "query_wise_intent_out": GuardrailsPath + "Archive/query_wise",
            "query_open_out": GuardrailsPath + "Archive/ProductCount/query_open",
            "query_weighted_out": GuardrailsPath
            + "Archive/ProductCount/query_weighted",
            "query_hard_out": GuardrailsPath + "Archive/ProductCount/query_hard",
            "topN_step_wise_path": GuardrailsPath + "run_" + str_date + "/topn_parquet",
            "guardrails_response": GuardrailsPath
            + "guardrails_response_template_queries.csv",
        },
        description="",
    )

    """
    Sanitizes the guardrails response and template queries and make it ready for analysis
    
    Guardrails response:
    input: query -> [res1,....res7] to 
    output: query -> {l1: l1_name, l2: l2_name, ....other entities}
    
    Template queries response:
    input: query -> [l1, l2] ---- parts from query and their respective values --- acts as ground truth
    output: query -> {l1: ..., l2: ...}
    """

    PrepareGuardrails = CouturePythonDockerOperator(
        task_id="PrepareGuardrails",
        image=python_image,
        api_version="auto",
        auto_remove=False,
        command="/bin/bash echo 'heloword'",
        extra_hosts=kerberos_hosts,
        user="couture",
        dag=dag,
        mem_limit="18g",
        code_artifact=code_artifact_python,
        python_deps=required_eggs,
        method_id="prepare_guardrails_for_analysis",
        method_args_dict={"top_n_to_process": 1000},
        volumes=[searchEngineVolume],
        input_base_dir_path="",
        output_base_dir_path="",
        input_filenames_dict={
            "guardrails_response_path": f"/data1/archive/rohith/{DATE}/Template_Queries_Guardrails/guardrails_response_template_queries.csv",
            "template_queries_path": "/data1/searchengine/EmbeddingTrainingDataCuration/ajio/09122024/V3/TemplatedQueriesFixed",
        },
        output_filenames_dict={
            "template_queries_output_path": f"/data1/archive/rohith/{DATE}/ProcessedTemplateQueries",
            "guardrails_response_output_path": f"/data1/archive/rohith/{DATE}/ProcessedGuardrailsResponse",
        },
        description="",
    )

    PrepareGuardrails.set_upstream(GetGuardrailsResponse)

    """
    Now both the responses are in: query -> {l1: l1_name, l2: l2_name, ....other entities} format --- ready for analysis
    
    Join the dataframes and do exact checks, synonym checks, and variant checks and returns the dataframe. 
    """
    AnalyzeGuardrails = CouturePythonDockerOperator(
        task_id="AnalyzeGuardrails",
        image=python_image,
        api_version="auto",
        auto_remove=False,
        command="/bin/bash echo 'heloword'",
        extra_hosts=kerberos_hosts,
        user="couture",
        dag=dag,
        mem_limit="18g",
        code_artifact=code_artifact_python,
        python_deps=required_eggs,
        method_id="analyze_guardrails_performance",
        method_args_dict={},
        volumes=[searchEngineVolume],
        input_base_dir_path="",
        output_base_dir_path="",
        input_filenames_dict={
            "guardrails_formatted_path": f"/data1/archive/rohith/{DATE}/ProcessedGuardrailsResponse",
            "template_queries_path": f"/data1/archive/rohith/{DATE}/ProcessedTemplateQueries",
            "variants_base_path": "/data1/searchengine/processed/ajio/24032025/V1_delta_changes",
        },
        output_filenames_dict={
            "base_guardrails_analysis_path": "/data1/archive/rohith/SearchAnalysis/28052025/GuardrailsAnalysis",
        },
        description="",
    )

    AnalyzeGuardrails.set_upstream(PrepareGuardrails)


################################################ GroundTruth Queries Testing  ##############################################################

with TaskGroup("GroundTruthAccuracyTesting", dag=dag) as GroundTruthAccuracyTesting:
    """
    Scala task to take the ground truth filters and apply it on the main catalogue to obtain: 
    output: query -> expected_products for all the 300-ish queries. 
    """

    ApplyFiltersOnCatalogue = CoutureSpark3Operator(
        task_id="ApplyFiltersOnCatalogue",
        class_path=classPath,
        method_id="ApplyFiltersOnCatalogue",
        code_artifact=code_artifact,
        method_args_dict={"": ""},
        input_base_dir_path="",
        output_base_dir_path="",
        input_filenames_dict={
            "catalogue_dataframe_path": f"/data1/searchengine/genai_search/ajio/processed/dev/24032025/V1/GenAIProcessedCatalogue",
            "product_interaction_dataframe_path": f"/data1/searchengine/genai_search/ajio/processed/dev/24032025/V1/ProductsDescription",
            "groundtruth_dataframe_path": f"/data1/archive/rohith/{DATE}/GroundTruthAnalysis/GT/gtClened-NormalizedOpenTokens.parquet",
        },
        output_filenames_dict={
            "ground_truth_analysis_base_path": f"/data1/archive/rohith/{DATE}/GroundTruthAnalysis-test/"
        },
        dag=dag,
        description="Apply the filters on the catalogue to get the expected products list",
    )

    """
    SSHOperator task: To obtain the predicted products (products from the API response)
    output: query -> received_products for all the 300-ish queries. 
    change the endpoint from the GPU conf file 
    """
    conda_env_command = "conda activate qdrant_env"

    # input variables for the task
    input_variables = {
        "api_endpoint": "http://10.166.181.219:8497/search-latest-stable",
        "queries_df_path": "/data1/archive/rohith/28042025/GroundTruthAnalysis/GT",
        "query_df_field": "query",
        "api_key": "ijMFtGnmttMz7Q.6hLailNlYzPRRg.MK2b83dYSedETA",
        "base_analysis_path": "/data1/archive/rohith/28042025/GroundTruthAnalysis",
        "top_n_to_process": 400,
    }

    retrieve_products_bash_command = f"""
    {conda_env_command} && sh /app/notebooks/Rohith/SEARCH_ANALYSIS_TESTING/RetrieveProductsCaller.sh \
        --api_endpoint "{input_variables["api_endpoint"]}" \
        --queries_df_path "{input_variables["queries_df_path"]}" \
        --query_df_field "{input_variables["query_df_field"]}" \
        --api_key "{input_variables["api_key"]}" \
        --base_analysis_path "{input_variables["base_analysis_path"]}" \
        --top_n_to_process "{input_variables["top_n_to_process"]}" \
    """

    RetrieveProducts = SSHOperator(
        task_id="RetrieveProducts",
        ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
        command=f"""ssh -p 8509 jioapp@10.166.181.219 "{retrieve_products_bash_command}" """,
        dag=dag,
    )

    RetrieveProducts.set_upstream(ApplyFiltersOnCatalogue)

    """
    Now we have the query -> ground_truth_products and query -> received_products
    Compare and generate the accuracy metrics based on the expected products. 
    """
    AnalyzeGT = CouturePythonDockerOperator(
        task_id="AnalyzeGT",
        image=python_image,
        api_version="auto",
        auto_remove=False,
        command="/bin/bash echo 'heloword'",
        extra_hosts=kerberos_hosts,
        user="couture",
        dag=dag,
        mem_limit="18g",
        code_artifact=code_artifact_python,
        python_deps=required_eggs,
        method_id="compare_api_against_ground_truth",
        method_args_dict={
            "gt_field": "expected_queries",
            "apidf_field": "received_products",
            "top_n_to_process": 400,
        },
        volumes=[searchEngineVolume],
        input_base_dir_path="",
        output_base_dir_path="",
        input_filenames_dict={
            "ground_truth_input_path": "/data1/archive/rohith/28042025/GroundTruthAnalysis/temp/finalDF",
            "received_products_input_path": "/data1/archive/rohith/28042025/GroundTruthAnalysis/APIRepsonses",
            "original_query_filters_path": "/data1/archive/rohith/28042025/GroundTruthAnalysis/GT/GroundTruth_300Queries.csv",
        },
        output_filenames_dict={
            "base_groundtruth_analysis_path": "/data1/archive/rohith/SearchAnalysis/28052025/GroundTruthAnalysis/",
        },
        description="",
    )

    AnalyzeGT.set_upstream(RetrieveProducts)


################################################ Ranking Analysis Scala task ##############################################################


RankingAnalysis = CoutureSpark3Operator(
    task_id="RankingAnalysis",
    class_path=classPath,
    method_id="RankingAnalysis",
    code_artifact="couture-search-pipelines-3.0.0-rohith.jar",
    method_args_dict={"": ""},
    input_base_dir_path="",
    output_base_dir_path="",
    input_filenames_dict={
        "history_df_path": f"/data1/searchengine/phase2/Dataset/02022025/V1/HistoryInteractionForNDCG",
        "query_product_scored_path": f"/data1/searchengine/testing/ajio/query_product_scored_top_100",
    },
    output_filenames_dict={
        "ranking_analysis_base_path": f"/data1/archive/rohith/{DATE}/RankingAnalysis/"
    },
    dag=dag,
    description="Do the ranking query wise",
)

################################################ Spell Check Analysis ##############################################################

with TaskGroup("SpellCheckAnalysis", dag=dag) as SpellCheckAnalysis:
    """
    Take in the mistake queries, corrected queries and original queries and calculate the leviathan distance between them.
    """

    SpellCheckSimpleAnalysis = CoutureSpark3Operator(
        task_id="SpellCheckSimpleAnalysis",
        class_path=classPath,
        method_id="SpellCheckSimpleAnalysis",
        code_artifact="couture-search-pipelines-3.0.0-rohith.jar",
        method_args_dict={
            # column names
            "original_query_column": "Ground Truth",
            "mistake_query_column": "Error Query",
            "corrected_query_column": "spell_corrected_query",
        },
        input_base_dir_path="",
        output_base_dir_path="",
        input_filenames_dict={
            "spellcheck_analysis_path": "/data1/archive/rohith/SearchAnalysis/28052025/SpellCheckAnalysis/SpellCorrectedQueriesCSV/",
            # "original_queries_df_path": f"/data1/archive/rohith/{DATE}/SpellCheckAnalysis/spell_check_test_data/part-00000-950222a9-a7f8-4816-b483-045268b4d75c-c000.csv",
        },
        output_filenames_dict={
            "spell_check_analysis_base_path": "/data1/archive/rohith/SearchAnalysis/28052025/SpellCheckAnalysis/"
        },
        dag=dag,
        description="Apply the filters on the catalogue to get the expected products list",
    )

    GetSpellCorrectedQueries = CouturePythonDockerOperator(
        task_id="GetSpellCorrectedQueries",
        image=python_image_new,
        api_version="auto",
        auto_remove=False,
        command="/bin/bash echo 'heloword'",
        extra_hosts=kerberos_hosts,
        user="couture",
        dag=dag,
        mem_limit="18g",
        code_artifact=code_artifact_python,
        python_deps=required_eggs,
        method_id="get_spell_corrected_queries",
        method_args_dict={
            # "top_n": 20,
            "mistake_query_column": "Error Query",
        },
        volumes=[searchEngineVolume],
        input_base_dir_path="",
        output_base_dir_path="",
        input_filenames_dict={
            "mistake_queries_path": "/data1/archive/rohith/SearchAnalysis/StaticData/SpellCheck/spell_correction_results_0904.csv",
            "spell_correct_model_path": "/data1/searchengine/phase2/TrainedModels/jiomart/ContextSpellCheckTrainedModelLMHistoryR2R",
            "spell_correct_model_path_local": "/home/jovyan/DownloadedModels",
        },
        output_filenames_dict={
            "spell_corrected_queries_path": "/data1/archive/rohith/SearchAnalysis/28052025/SpellCheckAnalysis/",
        },
        description="",
    )

    GetSpellCorrectedQueries >> SpellCheckSimpleAnalysis

    SingularizeSpellCheck = CouturePythonDockerOperator(
        task_id="SingularizeSpellCheckQueries",
        image=python_image_new,
        api_version="auto",
        auto_remove=False,
        command="/bin/bash echo 'heloword'",
        extra_hosts=kerberos_hosts,
        user="couture",
        dag=dag,
        mem_limit="18g",
        code_artifact=code_artifact_python,
        python_deps=required_eggs,
        method_id="singularize_spell_check_queries",
        method_args_dict={
            # "top_n": 20,
            "original_query_column": "Ground Truth",
            "mistake_query_column": "Error Query",
            "corrected_query_column": "spell_corrected_query",
        },
        volumes=[searchEngineVolume],
        input_base_dir_path="",
        output_base_dir_path="",
        input_filenames_dict={
            # "original_df_path": "/data1/archive/rohith/28042025/SpellCheckAnalysis/spell_check_test_data/part-00000-950222a9-a7f8-4816-b483-045268b4d75c-c000.csv",
            "results_df_path": "/data1/archive/rohith/SearchAnalysis/28052025/SpellCheckAnalysis/SpellCorrectedQueriesCSV",
        },
        output_filenames_dict={
            "singularized_queries_path": "/data1/archive/rohith/SearchAnalysis/28052025/SpellCheckAnalysis/SingularizedQueries",
        },
        description="",
    )

    SpellCheckSimpleAnalysis >> SingularizeSpellCheck

    SpellCheckSingularAnalysis = CoutureSpark3Operator(
        task_id="SpellCheckSingularAnalysis",
        class_path=classPath,
        method_id="SpellCheckSimpleAnalysis",
        code_artifact="couture-search-pipelines-3.0.0-rohith.jar",
        method_args_dict={
            # column names
            "original_query_column": "Ground Truth",
            "mistake_query_column": "Error Query",
            "corrected_query_column": "spell_corrected_query",
        },
        input_base_dir_path="",
        output_base_dir_path="",
        input_filenames_dict={
            "singularized_df_path": "/data1/archive/rohith/SearchAnalysis/28052025/SpellCheckAnalysis/SingularizedQueries/ResultsCSV/singularized_queries.csv",
        },
        output_filenames_dict={
            "spell_check_analysis_base_path": "/data1/archive/rohith/SearchAnalysis/28052025/SpellCheckAnalysis/SingularizedQueries/"
        },
        dag=dag,
        description="Apply the filters on the catalogue to get the expected products list",
    )

    SingularizeSpellCheck >> SpellCheckSingularAnalysis


################################################ Discount Cumulative Gain ##############################################################


CalculateDiscountCumulativeGain = CoutureSpark3Operator(
    task_id="CalcNDCG",
    class_path=classPath,
    method_id="CalculateDiscountCumulativeGain",
    code_artifact="couture-search-pipelines-3.0.0-rohith.jar",
    method_args_dict={"": ""},
    input_base_dir_path="",
    output_base_dir_path="",
    input_filenames_dict={
        "boosting_scores_df": "/data1/searchengine/genai_search/ajio/processed/dev/24032025/V1/ClickhouseAttributes",
        "point_id_mapping": "/data1/searchengine/genai_search/ajio/processed/dev/24032025/V1/GenAIProcessedCatalogue",
        # "qdrant_api_cache_path": "/data1/archive/searchengine/testing/ajio/query_product_scored",
        "qdrant_api_cache_path": "/data1/archive/rohith/28042025/QdrantCache/top10k_3005",
        
        "history_df_path": "/data1/searchengine/phase2/Dataset/02022025/V1/HistoryInteractionWithRatings",
        "scored_query_df_path": "/data1/searchengine/testing/ajio/query_product_scored_top_30",
    },
    output_filenames_dict={
        "discount_analysis_base_path": "/data1/archive/rohith/SearchAnalysis/28052025/NDCGAnalysisTop10k"
    },
    dag=dag,
    description="Do the ranking query wise",
)


################################################ Sanity Testing ##############################################################


with TaskGroup("BasicSanityChecking", dag=dag) as BasicSanityCheckingTasks:
    brand_testing_env_command = "conda activate /data/archita/searchengine_nlp"
    brand_testing_bash_command = textwrap.dedent(
        f"""
          {brand_testing_env_command} && sh /app/notebooks/Rohith/SEARCH_ANALYSIS_TESTING/SANITY_CHECK/SanityCheckerGeneralizedCaller.sh \
          --df_path "/data1/searchengine/processed/ajio/24032025/etl/Brands" \
          --endpoint "pre-validation" \
            --disable_spell_check "false" \
              --df_key "brandname" \
                --api_field "brandName_text_en_mv" \
                  --api_key "ijMFtGnmttMz7Q.6hLailNlYzPRRg.MK2b83dYSedETA" \
                    --base_sanity_analysis_output_path "/data1/archive/rohith/28042025/SanityCheckAnalysis/Brands" \
                          --min_count_required 25 \
                            --count_in_each_partition 10 \
                              --top_k 25
          """
    )

    brand_testing = SSHOperator(
        task_id="BrandsSanityAPITesting",
        ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
        command=f"""ssh -p 8509 jioapp@10.166.181.219 "{brand_testing_bash_command}" """,
        dag=dag,
    )

    category_testing_env_command = "conda activate /data/archita/searchengine_nlp"
    category_testing_bash_command = textwrap.dedent(
        f"""
          {category_testing_env_command} && sh /app/notebooks/Rohith/SEARCH_ANALYSIS_TESTING/SANITY_CHECK/SanityCheckerGeneralizedCaller.sh \
          --df_path "/data1/searchengine/processed/ajio/24032025/etl/CategoryCounts" \
          --endpoint "pre-validation" \
            --disable_spell_check "false" \
              --df_key "l1l3category_en_string_mv" \
                --api_field "l1l3category_en_string_mv" \
                  --api_key "ijMFtGnmttMz7Q.6hLailNlYzPRRg.MK2b83dYSedETA" \
                    --base_sanity_analysis_output_path "/data1/archive/rohith/28042025/SanityCheckAnalysis/Category" \
                          --min_count_required 25 \
                            --count_in_each_partition 10 \
                              --top_k 25 \
                                --partial_check_needed true
          """
    )

    category_testing = SSHOperator(
        task_id="CategorySanityAPITesting",
        ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
        command=f"""ssh -p 8509 jioapp@10.166.181.219 "{category_testing_bash_command}" """,
        dag=dag,
    )

    # Provide the upstream for the two tasks
    category_testing.set_upstream(brand_testing)

############################################### Cache Generation #################################################################

cache_generation_bash_command = f"""
{category_testing_env_command} & sh /app/notebooks/Rohith/SEARCH_ANALYSIS_TESTING/SANITY_CHECK/CacheGeneratorCaller.sh \
  --spell_check_df_path "/data1/archive/rohith/SearchAnalysis/StaticData/SpellCorrectedTopQueries" \
  --endpoint "pre-validation" \
  --api_key "ijMFtGnmttMz7Q.6hLailNlYzPRRg.MK2b83dYSedETA" \
  --cache_type "Qdrant" \
  --query_slice_start 0 \
  --query_slice_end 1 \
  --done_base_dir "/app/notebooks/Rohith/SEARCH_ANALYSIS_TESTING/SANITY_CHECK/CACHE_GENERATION" \
  --failed_base_dir "/app/notebooks/Rohith/SEARCH_ANALYSIS_TESTING/SANITY_CHECK/CACHE_GENERATION" \
  --done_dir "" \
  --qdrant_top_k 450 \
  --batch_size 10 \
  --output_path "/data1/archive/rohith/tmp/QdrantCache"
"""

cache_generation = SSHOperator(
        task_id="CacheGeneration",
        ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
        command=f"""ssh -p 8509 jioapp@10.166.181.219 "{cache_generation_bash_command}" """,
        dag=dag,
    )
    

############################################### SearchStability #################################################################
"""
In this tasks the inputs are 
1. Model
2. UKUS, Hinglish and Shuffled queries data merged into one , here we have three columns
   query,mistake_queries,category

the output will be a csv report with the metrics on simmilarity score the model produces on the two query columns
"""
bash_command = textwrap.dedent(
    f"""
    {env_command} && sh /app/notebooks/Rohith/SEARCH_ANALYSIS_TESTING/SEARCH_STABILITY/EmbeddingsCompare.sh \
    --model_path "{modelPath}" \
    --query_file_path "/data1/archive/avinash/ModelTestingData2/inputs/" \
    --report_csv_path "/data1/archive/rohith/SearchAnalysis/28052025/SearchStability/Results/SearchStabilityReport.csv" \
    --query_col1 "query" \
    --query_col2 "mistake_queries" \
    --n 200000 \
    --model_name "mpnet_v3_ep3" \
    --batch_size 1024 \
    --step_size 100000 \
    --multi_gpu True
    """
)
SearchStability = SSHOperator(
    task_id="SearchStability",
    ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
    command=f"""ssh -p 8509 jioapp@10.166.181.219 "{bash_command}" """,
    dag=dag,
)

# SearchStability.set_upstream(SpellCheck)

########################################### ZSR analysis #####################################################################
"""
In this tasks from qdrant cache , we get the queries which give zero resulsts that is zsr
the inputs are :-
1.Dataframe with query and backend response column in json( dumped into string )
2.a key in the string json , which should be array and this checks for empty array
the output will be :-
1.Dataframe with same schema but only zsr queries
"""

with TaskGroup("ZSRAnalysisTasks", dag=dag) as ZSRAnalysisTasks:
    AnalyseZSRold = None
    for experiment in ["default", "non_premium", "premium"]:
        AnalyseZSRnew = CoutureSpark3Operator(
            task_id=f"AnalyseZSR.{experiment}",
            class_path=classPath,
            method_id="AnalyseZSR",
            code_artifact="couture-search-pipelines-3.0.0-rohith.jar",
            method_args_dict={
                "search_response_column": "response",
                "key_to_check": "top_code_strings",
                "column_name": "brand_string_mv",
            },
            input_base_dir_path="",
            output_base_dir_path="",
            input_filenames_dict={"qdrant_cache": "/data1/archive/rohith/SearchAnalysis/28052025/QdrantCache/default"},
            output_filenames_dict={"zsr_queries": "/data1/archive/rohith/SearchAnalysis/28052025/ZSRAnalysis/Results"},
            dag=dag,
            description="filters zsr queries from the qdrant cache",
        )
        if AnalyseZSRold is not None:
            AnalyseZSRnew.set_upstream(AnalyseZSRold)
        AnalyseZSRold = AnalyseZSRnew

########################################### CTR analysis #####################################################################
"""
In this task, we get the queries from qdrant cache which have click-through rate (CTR) data.
The inputs are:-
1. Dataframe with query and backend response column in json (dumped into string)
2. A key in the string json, which should contain the CTR data
The output will be:-
1. Dataframe with the same schema but only queries that have valid CTR data.
"""

with TaskGroup("CTRAnalysisTasks", dag=dag) as CTRAnalysisTasks:
    AnalyseCTRold = None
    for experiment in ["default", "non_premium", "premium"]:
        AnalyseCTRnew = CoutureSpark3Operator(
            task_id=f"AnalyseCTR.{experiment}",
            class_path=classPath,
            method_id="AnalyseCTR",
            code_artifact=code_artifact,
            method_args_dict={
                "search_term_column": "query",
                "product_id_column": "option_code",
            },
            input_base_dir_path="/data1/archive/avinash/QdrantCache",
            output_base_dir_path=testDataPath,
            input_filenames_dict={
                "history_data": "interactions",
                "console_data": f"{experiment}_top5000_onlyCodeStrings",
                "top_percentile": 0.95,
                "top_n": 5000,
            },
            output_filenames_dict={"ctr_queries": f"/CTR_reports/{experiment}"},
            dag=dag,
            description="Gives the average ctr at query level",
        )
        if AnalyseCTRold is not None:
            AnalyseCTRnew.set_upstream(AnalyseCTRold)
        AnalyseCTRold = AnalyseCTRnew

######################################################### Gaurdrails Brand Testing and Template testing ################################
# gpu commands
# brand_testing_env_command = "conda activate /data/archita/searchengine_nlp"
# brand_testing_bash_command = textwrap.dedent(
#         f"""
#         {brand_testing_env_command} && sh /app/notebooks/avinash/Gaurdrails_task/Brand-Testing/build.sh \
#         --brands_path "/data1/searchengine/processed/ajio/24032025/etl/Brands" \
#   		--endpoint "latest_stable" \
#   		--disable_spell_check "True" \
#         --ouput_path "/data1/archive/rohith/28042025/SanityChecks/BrandSanityChecks/"
#         """
# )


# brand_testing = SSHOperator(
#     task_id="Brands_Testing",
#     ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
#     command=f"""ssh -p 8509 jioapp@10.166.181.219 "{brand_testing_bash_command}" """,
#     dag=dag
# )

# template_testing_env_command = "conda activate /home/jioapp/anaconda3/envs/searchengine_phase2"
# template_testing_bash_command = textwrap.dedent(
#         f"""
#         {template_testing_env_command} && sh /app/notebooks/avinash/Gaurdrails_task/Template-Testing/ajio/Template_Testing.sh \
#         --templates_path "/data1/archive/avinash/GuardRails_TemplateQueries" \
# 		--output_base_path "/data1/archive/avinash/Template-Testing/ajio" \
#         --vertical "ajio" \
# 		--phase "dev09022025_1802" \
# 		--top_n 15
#         """
# )

# template_testing = SSHOperator(
#     task_id="Template_Testing",
#     ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
#     command=f"""ssh -p 8509 jioapp@10.166.181.219 "{template_testing_bash_command}" """,
#     dag=dag
# )

####################avinash#################################Gaurdrails and jiomart###############################################################

# verticals=['groceries','beauty','fashion','home_lifestyle','other_verticals','electronics']

# vertical_tasks=[]
# for vertical in verticals:
#     jiomart_template_testing_env_command = "conda activate /home/jioapp/anaconda3/envs/searchengine_phase2"
#     jiomart_template_testing_bash_command = textwrap.dedent(
#             f"""
#             {jiomart_template_testing_env_command} && sh /app/notebooks/avinash/Gaurdrails_task/Template-Testing/jiomart/Template_Testing.sh \
#             --templates_path "/data1/searchengine/testing/jiomart/Templates_testing/{vertical}/Input" \
#             --output_base_path "/data1/searchengine/testing/jiomart/Templates_testing/{vertical}" \
#             --vertical "{vertical}" \
#             --phase "dev0509_1003:{vertical}" \
#             --top_n 15
#             """
#     )

#     jiomart_template_testing = SSHOperator(
#         task_id=f"Jiomart_Template_Testing.{vertical}",
#         ssh_conn_id="AIRFLOW_CONN_SSH_SERVER",
#         command=f"""ssh -p 8509 jioapp@10.166.181.219 "{jiomart_template_testing_bash_command}" """,
#         dag=dag
#     )

#     if len(vertical_tasks)==0:
#       template_testing >> jiomart_template_testing
#     else:
#       vertical_tasks[-1] >> jiomart_template_testing

#     vertical_tasks.append(jiomart_template_testing)

# brand_testing >> template_testing


"""
Final response -- curate all the responses into 1
"""

pre_validation_base_path = (
    "/data1/archive/rohith/SearchAnalysis/28052025/pre-validation"
)
latest_stable_base_path = ""

selected = pre_validation_base_path

CompileTestingAnalysisReport = CouturePythonDockerOperator(
    task_id="CompileTestingAnalysisReport",
    image=python_image,
    api_version="auto",
    auto_remove=False,
    command="/bin/bash echo 'heloword'",
    extra_hosts=kerberos_hosts,
    user="couture",
    dag=dag,
    mem_limit="18g",
    code_artifact=code_artifact_python,
    python_deps=required_eggs,
    method_id="compile_testing_analysis_report",
    method_args_dict={
        "gt_field": "expected_queries",
        "apidf_field": "received_products",
        "top_n_to_process": 400,
    },
    volumes=[searchEngineVolume],
    input_base_dir_path="",
    output_base_dir_path="",
    input_filenames_dict={
        # basic sanity DFs
        "sanity_brand_df": f"{selected}/SanityCheckAnalysis/Brands",
        "sanity_category_df": f"{selected}/SanityCheckAnalysis/Category",
        # ground truth DFs
        "total_gtdf": f"{selected}/GroundTruthAnalysis/AnalysisResults/TOTAL",
        "top_100_gtdf": f"{selected}/GroundTruthAnalysis/AnalysisResults/top_100",
        # guardrails analysis DFs:
        "guardrails_exact": f"{selected}/GuardrailsAnalysis/exact_matching_errors",
        "guardrails_synonyms": f"{selected}/GuardrailsAnalysis/synonym_errors",
        "guardrails_variants": f"{selected}/GuardrailsAnalysis/variant_errors",
        # spellcheck analysis DFs:
        "spell_check_original": f"{selected}/SpellCheckAnalysis/AnalysisResults",
        "spell_check_singular": f"{selected}/SpellCheckAnalysis/SingularizedQueries/AnalysisResults",
        # Search stability:
        "search_stability": f"{selected}/SearchStability/Results/SearchStabilityReport.csv",
        # Expected CTR DFs:
        "expected_ctr": f"{selected}/ModelTestingData2/CTR",
        # Discount Cumulative Gains:
        "discount_cumulative_gains": f"{selected}/CumulativeDiscountAnalysis/NDCG_Top_30/part-00000-ab652023-6d7a-4666-9358-531e450ff3eb-c000.csv",
        # Ranking score correlations:
        "ranking_score_correlation": f"{selected}/RankingAnalysis/summaryStatsDF/part-00000-75ff8475-5571-436e-b9f0-561a193727a5-c000.csv",
        # Expected ZSR:
        "expected_zsr_base_path": f"{selected}/ZSRQueries/top10k/default",
    },
    output_filenames_dict={
        "analysis_report": f"{selected}/FINAL_PIPELINE_OUTPUT/",
    },
    description="",
)


"""
Make sure the coalating task runs only after all the tasks are completed
"""

CompileTestingAnalysisReport.set_upstream(
    [
        BasicSanityCheckingTasks,
        CTRAnalysisTasks,
        CalculateDiscountCumulativeGain,
        GroundTruthAccuracyTesting,
        GuardrailsAnalysis,
        RankingAnalysis,
        SearchStability,
        SpellCheckAnalysis,
        ZSRAnalysisTasks,
    ]
)
